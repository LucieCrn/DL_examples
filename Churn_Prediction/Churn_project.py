# -*- coding: utf-8 -*-
"""Deep Learning Churn Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H6nKsJbMN9lw1OFsac7p1Oz_vUAodpUK
"""

## CHURN PROJECT ##

# Load libraries 
import pandas as pd 
from sklearn import pipeline
from sklearn.datasets import make_classification
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier 
from sklearn import datasets
# Import train_test_split function 
from sklearn.model_selection import train_test_split
# Import scikit-learn metrics module for accuracy calculation 
from sklearn import metrics 
# Import numpy metrics module 
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score
# Import plot package
import matplotlib.pyplot as plt 
import seaborn as sns

data = pd.read_csv('/content/TelecomChurnDataset.csv')

data["TotalCharges"] = pd.to_numeric(data.TotalCharges, errors='coerce')

data['TotalCharges'].astype(float)

data.info()

data.describe()

# We keep a version of the dataset without dummies for practical purposes for datavisualization
data_nodummies = data
# We create dummy variables for the qualitative variables
data = pd.get_dummies(data, columns=['gender', 'Partner', 'Dependents','PhoneService', 'MultipleLines', 'InternetService','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling','PaymentMethod', 'Churn'])

# Preparation steps
data = data.replace(r'^\s*$', np.nan, regex=True)
data = data.dropna(axis=0)

data.info()

data.describe()

data.head

data.columns

features = data.drop(["Churn_No", "Churn_Yes", "customerID"], axis=1) #churn = target variable

X = np.array(features)

y = np.array(data["Churn_No"])

### Descriptive graphs

fig = plt.figure(figsize=(10, 6)) 
ax = fig.add_subplot(111)

# proportion of observation of each class
prop_response = data['Churn_Yes'].value_counts(normalize=True)

# create a bar plot showing the percentage of churn
prop_response.plot(kind='bar', 
                   ax=ax,
                   color=['green','red'])

# set title and labels
ax.set_title('Proportion of churning observations')
ax.set_xlabel('Churn',)
ax.set_ylabel('Proportions of observations')
ax.tick_params(rotation='auto')

# histogram the distribution of tenure.
tenure_hist = plt.hist(data = data, x = 'tenure');
# set title and labels
plt.xlabel('Tenure')
plt.ylabel('Number of observations')
plt.title("Distribution of Tenure")



data["MonthlyCharges"].plot(kind='density', subplots=True, layout=(1,2), 
                  sharex=False, figsize=(8,4));
plt.xlabel('MontlyCharges')
plt.title("Density plot of MontlyCharges")



data["TotalCharges"].plot(kind='density', subplots=True, layout=(1,2), 
                  sharex=False, figsize=(8,4));

print(pd.crosstab(data_nodummies.gender, data_nodummies.Churn, margins=True))
pd.crosstab(data_nodummies.gender, data_nodummies.Churn, margins=True).plot(kind="bar",figsize=(10,5))
plt.title("Churn by Gender")

print(pd.crosstab(data_nodummies.Contract, data_nodummies.Churn, margins=True))
pd.crosstab(data_nodummies.Contract, data_nodummies.Churn, margins=True).plot(kind="bar",figsize=(10,5))
plt.title("Churn by Contract type")

print(pd.crosstab(data_nodummies.InternetService, data_nodummies.Churn, margins=True))
pd.crosstab(data_nodummies.InternetService, data_nodummies.Churn, margins=True).plot(kind="bar",figsize=(10,5))
plt.title("Churn by InternetService feature")

print(pd.crosstab(data_nodummies.StreamingMovies, data_nodummies.Churn, margins=True))
pd.crosstab(data_nodummies.StreamingMovies, data_nodummies.Churn, margins=True).plot(kind="bar",figsize=(10,5))
plt.title("Churn by StreamingMovies feature")

fig1, ax1 = plt.subplots()
ax1.set_title('Total Charges Box Plot')
ax1.boxplot(data["TotalCharges"],notch=True)



# Plot correlation matrix 
plt.figure(figsize = (30,20))        # Setting up the size of the figure
sns.heatmap(data.corr(),annot = True)

# Plot correlation matrix, without higly correlated variables
data_low_corr = data.drop(["OnlineSecurity_No","OnlineBackup_No","TechSupport_No","StreamingTV_No","StreamingMovies_No","InternetService_No","OnlineSecurity_No internet service","DeviceProtection_No internet service", "StreamingTV_No internet service", "StreamingMovies_No internet service","OnlineBackup_No internet service", "TechSupport_No internet service","Churn_No","Churn_Yes","Partner_No","Dependents_No", "PhoneService_Yes", "DeviceProtection_No","MultipleLines_No"],1)
plt.figure(figsize = (20,20))        # Setting up the size of the figure
sns.heatmap(data_low_corr.corr(),annot = True)

### 1st Model Logistic Regression with Bagging Classifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% training and 20% test

y_train.view()

X, y = make_classification(random_state=50)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
pipe = make_pipeline(StandardScaler(), LogisticRegression())
pipe.fit(X_train, y_train)  # apply scaling on training data
# pipeline(steps=[('standardscaler', StandardScaler()), ('logisticregression', LogisticRegression())])

pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.

# Create Bagging Classifier Object
lgc = LogisticRegression()
LogBag = BaggingClassifier(base_estimator = lgc, n_estimators=100)
# Train Bagging Classifer
model1 = LogBag.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = model1.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

### Confusion Matrix 
y_pred = model1.predict(X_test)
y_pred = y_pred > 0.5
print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))
print("Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred)*100))

### 2nd Model Decision Tree

# Create Bagging classifer object
dt = DecisionTreeClassifier()
dtBag = BaggingClassifier(base_estimator= dt, n_estimators=50)
# Train Bagging Classifer
model = dtBag.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = model.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

features = data.drop(["Churn_No", "Churn_Yes", "customerID"], axis=1) #churn = target variable

X = np.array(features)

y = np.array(data["Churn_No"])

X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.20)

tree = DecisionTreeClassifier() 
tree.fit(X_train, y_train)

tree.tree_.max_depth

validation_prediction =  tree.predict(X_val)
training_prediction = tree.predict(X_train)

from sklearn.metrics import accuracy_score

print('Accuracy training set:', accuracy_score(y_true=y_train, y_pred=training_prediction))
print('Accuracy validation set:', accuracy_score(y_true=y_val, y_pred=validation_prediction))

#constraining the model

tree = DecisionTreeClassifier(min_samples_leaf=10, max_depth=6, min_samples_split=25)

tree.fit(X_train, y_train)
validation_prediction = tree.predict(X_val)
training_prediction = tree.predict(X_train)

print('Accuracy training set: ', accuracy_score(y_true=y_train, y_pred=training_prediction))
print('Accuracy validation set: ', accuracy_score(y_true=y_val, y_pred=validation_prediction))

import graphviz
from sklearn.tree import export_graphviz

!apt install libgraphviz-dev
!pip install pygraphviz

feature_names = features.columns

dot_data = export_graphviz(tree, out_file=None,
                           feature_names=feature_names,
                           class_names=True,
                           filled=True, rounded=True,
                           special_characters=True)
graph = graphviz.Source(dot_data)
graph

### 3rd Model SVC (100% accuracy = not good)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

from matplotlib import pyplot as plt
# %matplotlib inline

data.head()

X = data.drop(['Churn_No', 'customerID'], axis='columns')
X.head()

data = data.replace(r'^\s*$', np.nan, regex=True)
data = data.dropna(axis=0)

y = data.Churn_No

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

print(len(X_train))
print(len(X_test))

model = SVC(kernel='linear')

model.fit(X_train, y_train)

predictions = model.predict(X_test)
print(predictions)

percentage = model.score(X_test, y_test)

from sklearn.metrics import confusion_matrix
res = confusion_matrix(y_test, predictions)
print("Confusion matrix")
print(res)
print(f"Test Set: {len(X_test)}")
print(f"Accuracy: {percentage*100}%")

# As we can see, the model is overfited. Thus, we are able to use BaggingClassifier in order to deal with overfitting

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

abc = BaggingClassifier(n_estimators=20)
model = abc.fit(X_train, y_train)

# Create Bagging classifier object
svc = SVC()
svcBag = BaggingClassifier(base_estimator= svc, n_estimators=20)

# Train Bagging Classifier
model1 = svcBag.fit(X_train, y_train)

# Predict the response for test dataset
y_pred = model1.predict(X_test)

# Model Accuracy, how often is the classifier correct ?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

### 4th Model Random Forest

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score

classifier = RandomForestClassifier()
classifier.fit(X_train, y_train)

data = pd.read_csv("/content/TelecomChurnDataset.csv")

# We create dummy variables for the qualitative variables
data = pd.get_dummies(data, columns=['gender', 'Partner', 'Dependents','PhoneService', 'MultipleLines', 'InternetService','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling','PaymentMethod', 'Churn'])

data = data.replace(r'^\s*$', np.nan, regex=True)
data = data.dropna(axis=0)

features = data.drop(["Churn_No", "Churn_Yes", "customerID"], axis=1) #churn = target variable

X = np.array(features)

y = np.array(data["Churn_No"])

X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.20)

classifier = RandomForestClassifier()

classifier.fit(X_train, y_train)

validation_prediction = classifier.predict(X_val)
training_prediction = classifier.predict(X_train)

print('Accuracy training set: ', accuracy_score(y_true=y_train, y_pred=training_prediction))
print('Accuracy validation set: ', accuracy_score(y_true=y_val, y_pred=validation_prediction))

classifier = RandomForestClassifier(min_samples_leaf=10, max_depth=4, min_samples_split=50) #first try

classifier.fit(X_train, y_train)
validation_prediction = classifier.predict(X_val)
training_prediction = classifier.predict(X_train)

print('Accuracy training set: ', accuracy_score(y_true=y_train, y_pred=training_prediction))
print('Accuracy validation set: ', accuracy_score(y_true=y_val, y_pred=validation_prediction))

classifier = RandomForestClassifier(min_samples_leaf=10, max_depth=6, min_samples_split=50) #Change max_depth to improve the model

classifier.fit(X_train,y_train)
validation_prediction = classifier.predict(X_val)
training_prediction = classifier.predict(X_train)

print('Accuracy training set: ', accuracy_score(y_true=y_train, y_pred=training_prediction))
print('Accuracy validation set: ', accuracy_score(y_true=y_val, y_pred=validation_prediction))

classifier = RandomForestClassifier(min_samples_leaf=4, max_depth=14, min_samples_split=20) # Change all the parameters to improve accuracy

classifier.fit(X_train,y_train)
validation_prediction = classifier.predict(X_val)
training_prediction = classifier.predict(X_train)

print('Accuracy training set: ', accuracy_score(y_true=y_train, y_pred=training_prediction))
print('Accuracy validation set: ', accuracy_score(y_true=y_val, y_pred=validation_prediction))

### Stacking Classifier

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlxtend --upgrade

from sklearn import model_selection
from mlxtend.classifier import StackingClassifier
import warnings

clf1 = RandomForestClassifier(random_state=1)
clf2 = SVC()
lr = LogisticRegression()
sclf = StackingClassifier(classifiers=[clf1, clf2], meta_classifier=lr)

# Commented out IPython magic to ensure Python compatibility.
print('3-fold cross validation:\n')

for clf, label in zip([clf1, clf2, sclf],
                      ['Random Forest',
                       'SVC',
                       'StackingClassifier']):
  scores = model_selection.cross_val_score(clf, X, y,
                                           cv=2, scoring='accuracy')
  print("Accuracy: %0.2f (+/- %0.2f) [%s]"
#   % (scores.mean(), scores.std(), label))
